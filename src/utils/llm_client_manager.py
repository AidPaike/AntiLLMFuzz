"""LLMå®¢æˆ·ç«¯ç®¡ç†å™¨ - æ”¯æŒå¤šåç«¯å’Œä¼˜é›…é™çº§"""

import logging
import requests
from typing import List, Dict, Any, Optional
from abc import ABC, abstractmethod

logger = logging.getLogger(__name__)


class BaseLLMClient(ABC):
    """LLMå®¢æˆ·ç«¯åŸºç±»"""
    
    def __init__(self, name: str):
        self.name = name
        self.available = False
    
    @abstractmethod
    def test_connection(self) -> bool:
        """æµ‹è¯•è¿æ¥æ˜¯å¦å¯ç”¨"""
        pass
    
    @abstractmethod
    def generate(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        pass


class OllamaClient(BaseLLMClient):
    """Ollamaæœ¬åœ°å®¢æˆ·ç«¯"""
    
    def __init__(self, model: str = "qwen2.5-coder", endpoint: str = "http://localhost:11434"):
        super().__init__("Ollama")
        self.model = model
        self.endpoint = endpoint
        self.available = self.test_connection()
    
    def test_connection(self) -> bool:
        """æµ‹è¯•Ollamaè¿æ¥"""
        try:
            response = requests.get(f"{self.endpoint}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [m['name'] for m in models]
                if any(self.model in name for name in model_names):
                    logger.info(f"âœ… Ollamaå¯ç”¨ï¼Œæ¨¡å‹: {self.model}")
                    return True
                else:
                    logger.warning(f"âš ï¸ Ollamaå¯ç”¨ä½†ç¼ºå°‘æ¨¡å‹: {self.model}")
                    logger.info(f"ğŸ’¡ å¯ç”¨æ¨¡å‹: {model_names}")
                    return False
        except Exception as e:
            logger.debug(f"Ollamaè¿æ¥å¤±è´¥: {e}")
        return False
    
    def generate(self, prompt: str, **kwargs) -> str:
        """ä½¿ç”¨Ollamaç”Ÿæˆæ–‡æœ¬"""
        if not self.available:
            raise RuntimeError("Ollamaä¸å¯ç”¨")
        
        try:
            data = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                **kwargs
            }
            
            response = requests.post(
                f"{self.endpoint}/api/generate",
                json=data,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            return result.get('response', '')
            
        except Exception as e:
            logger.error(f"Ollamaç”Ÿæˆå¤±è´¥: {e}")
            raise


class MockLLMClient(BaseLLMClient):
    """æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯ - ç”¨äºæµ‹è¯•å’Œæ¼”ç¤º"""
    
    def __init__(self):
        super().__init__("Mock")
        self.available = True
    
    def test_connection(self) -> bool:
        return True
    
    def generate(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿå“åº”"""
        # æ ¹æ®æç¤ºè¯ç±»å‹ç”Ÿæˆä¸åŒçš„æ¨¡æ‹Ÿå“åº”
        if "test case" in prompt.lower():
            return self._generate_mock_test_case()
        elif "security" in prompt.lower():
            return self._generate_mock_security_test()
        elif "edge case" in prompt.lower():
            return self._generate_mock_edge_case()
        else:
            return self._generate_generic_response()
    
    def _generate_mock_test_case(self) -> str:
        return """
// Mock test case
public class TestExample {
    public void testBasicFunctionality() {
        String input = "test input";
        String result = processInput(input);
        assertEquals("expected", result);
    }
}
"""
    
    def _generate_mock_security_test(self) -> str:
        return """
// Mock security test
public class SecurityTest {
    public void testInputValidation() {
        String maliciousInput = "<script>alert('xss')</script>";
        assertThrows(SecurityException.class, () -> {
            processUntrustedInput(maliciousInput);
        });
    }
}
"""
    
    def _generate_mock_edge_case(self) -> str:
        return """
// Mock edge case test
public class EdgeCaseTest {
    public void testNullInput() {
        assertThrows(IllegalArgumentException.class, () -> {
            processInput(null);
        });
    }
    
    public void testEmptyInput() {
        String result = processInput("");
        assertNotNull(result);
    }
}
"""
    
    def _generate_generic_response(self) -> str:
        return "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„LLMå“åº”ã€‚åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œè¿™é‡Œä¼šæ˜¯çœŸå®çš„AIç”Ÿæˆå†…å®¹ã€‚"


class OpenAIClient(BaseLLMClient):
    """OpenAI APIå®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo", base_url: Optional[str] = None):
        super().__init__("OpenAI")
        self.api_key = api_key
        self.model = model
        self.base_url = base_url or "https://api.openai.com/v1"
        self.available = self.test_connection() if api_key else False
    
    def test_connection(self) -> bool:
        """æµ‹è¯•OpenAI APIè¿æ¥"""
        if not self.api_key:
            return False
        
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            response = requests.get(
                f"{self.base_url}/models",
                headers=headers,
                timeout=10
            )
            
            if response.status_code == 200:
                logger.info("âœ… OpenAI APIå¯ç”¨")
                return True
            else:
                logger.warning(f"âš ï¸ OpenAI APIå“åº”å¼‚å¸¸: {response.status_code}")
                return False
                
        except Exception as e:
            logger.debug(f"OpenAI APIè¿æ¥å¤±è´¥: {e}")
            return False
    
    def generate(self, prompt: str, **kwargs) -> str:
        """ä½¿ç”¨OpenAI APIç”Ÿæˆæ–‡æœ¬"""
        if not self.available:
            raise RuntimeError("OpenAI APIä¸å¯ç”¨")
        
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": self.model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": kwargs.get("max_tokens", 1000),
                "temperature": kwargs.get("temperature", 0.7)
            }
            
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            return result["choices"][0]["message"]["content"]
            
        except Exception as e:
            logger.error(f"OpenAI APIç”Ÿæˆå¤±è´¥: {e}")
            raise


class LLMClientManager:
    """LLMå®¢æˆ·ç«¯ç®¡ç†å™¨ - æ”¯æŒå¤šåç«¯å’Œè‡ªåŠ¨é™çº§"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.clients: List[BaseLLMClient] = []
        self.current_client: Optional[BaseLLMClient] = None
        self._init_clients()
    
    def _init_clients(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„LLMå®¢æˆ·ç«¯"""
        logger.info("ğŸ” åˆå§‹åŒ–LLMå®¢æˆ·ç«¯...")
        
        # 1. å°è¯•Ollama (æœ¬åœ°ä¼˜å…ˆ)
        try:
            ollama_config = self.config.get('ollama', {})
            model = ollama_config.get('model', 'qwen2.5-coder')
            endpoint = ollama_config.get('endpoint', 'http://localhost:11434')
            
            ollama_client = OllamaClient(model, endpoint)
            self.clients.append(ollama_client)
            
            if ollama_client.available:
                logger.info(f"âœ… Ollamaå®¢æˆ·ç«¯å·²å°±ç»ª: {model}")
            else:
                logger.info("ğŸ’¡ Ollamaä¸å¯ç”¨ï¼Œå°è¯•å…¶ä»–é€‰é¡¹...")
                
        except Exception as e:
            logger.debug(f"Ollamaåˆå§‹åŒ–å¤±è´¥: {e}")
        
        # 2. å°è¯•OpenAI API
        try:
            openai_config = self.config.get('openai', {})
            api_key = openai_config.get('api_key') or self.config.get('api_key')
            
            if api_key and api_key != "${OPENAI_API_KEY}":
                model = openai_config.get('model', 'gpt-3.5-turbo')
                base_url = openai_config.get('base_url')
                
                openai_client = OpenAIClient(api_key, model, base_url)
                self.clients.append(openai_client)
                
                if openai_client.available:
                    logger.info(f"âœ… OpenAIå®¢æˆ·ç«¯å·²å°±ç»ª: {model}")
                    
        except Exception as e:
            logger.debug(f"OpenAIåˆå§‹åŒ–å¤±è´¥: {e}")
        
        # 3. æ€»æ˜¯æ·»åŠ Mockå®¢æˆ·ç«¯ä½œä¸ºå…œåº•
        mock_client = MockLLMClient()
        self.clients.append(mock_client)
        logger.info("âœ… Mockå®¢æˆ·ç«¯å·²å°±ç»ª (å…œåº•æ–¹æ¡ˆ)")
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„å®¢æˆ·ç«¯
        self._select_best_client()
    
    def _select_best_client(self):
        """é€‰æ‹©æœ€ä½³å¯ç”¨å®¢æˆ·ç«¯"""
        for client in self.clients:
            if client.available:
                self.current_client = client
                logger.info(f"ğŸ¯ é€‰æ‹©LLMå®¢æˆ·ç«¯: {client.name}")
                return
        
        # å¦‚æœæ²¡æœ‰å¯ç”¨å®¢æˆ·ç«¯ï¼Œä½¿ç”¨Mock
        self.current_client = self.clients[-1]  # Mockå®¢æˆ·ç«¯
        logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„LLMå®¢æˆ·ç«¯ï¼Œä½¿ç”¨Mockæ¨¡å¼")
    
    def generate(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆæ–‡æœ¬ï¼Œè‡ªåŠ¨å¤„ç†å¤±è´¥å’Œé‡è¯•"""
        if not self.current_client:
            raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„LLMå®¢æˆ·ç«¯")
        
        # å°è¯•å½“å‰å®¢æˆ·ç«¯
        try:
            return self.current_client.generate(prompt, **kwargs)
        except Exception as e:
            logger.warning(f"å½“å‰å®¢æˆ·ç«¯ {self.current_client.name} å¤±è´¥: {e}")
            
            # å°è¯•é™çº§åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨å®¢æˆ·ç«¯
            return self._fallback_generate(prompt, **kwargs)
    
    def _fallback_generate(self, prompt: str, **kwargs) -> str:
        """é™çº§ç”Ÿæˆ - å°è¯•å…¶ä»–å¯ç”¨å®¢æˆ·ç«¯"""
        if not self.current_client:
            raise RuntimeError("æ²¡æœ‰å½“å‰å®¢æˆ·ç«¯")
            
        current_index = self.clients.index(self.current_client)
        
        for i in range(current_index + 1, len(self.clients)):
            client = self.clients[i]
            if client.available:
                try:
                    logger.info(f"ğŸ”„ é™çº§åˆ°å®¢æˆ·ç«¯: {client.name}")
                    self.current_client = client
                    return client.generate(prompt, **kwargs)
                except Exception as e:
                    logger.warning(f"é™çº§å®¢æˆ·ç«¯ {client.name} ä¹Ÿå¤±è´¥: {e}")
                    continue
        
        # æœ€åå°è¯•Mockå®¢æˆ·ç«¯
        mock_client = self.clients[-1]
        logger.warning("ğŸ”„ é™çº§åˆ°Mockå®¢æˆ·ç«¯")
        self.current_client = mock_client
        return mock_client.generate(prompt, **kwargs)
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰å®¢æˆ·ç«¯çŠ¶æ€"""
        return {
            'current_client': self.current_client.name if self.current_client else None,
            'available_clients': [
                {
                    'name': client.name,
                    'available': client.available
                }
                for client in self.clients
            ]
        }
    
    def is_mock_mode(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åœ¨Mockæ¨¡å¼"""
        return isinstance(self.current_client, MockLLMClient)


# å…¨å±€å®ä¾‹
_llm_manager = None

def get_llm_manager(config: Optional[Dict[str, Any]] = None) -> LLMClientManager:
    """è·å–å…¨å±€LLMç®¡ç†å™¨å®ä¾‹"""
    global _llm_manager
    if _llm_manager is None:
        _llm_manager = LLMClientManager(config)
    return _llm_manager


def reset_llm_manager():
    """é‡ç½®LLMç®¡ç†å™¨ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    global _llm_manager
    _llm_manager = None