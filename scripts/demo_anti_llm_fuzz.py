#!/usr/bin/env python3
"""ååˆ¶æ•ˆæœæ¼”ç¤ºè„šæœ¬ - å¯¹æ¯”åŸå§‹æ–‡æ¡£å’Œæ‰°åŠ¨æ–‡æ¡£çš„LLMæ¨¡ç³Šæµ‹è¯•æ•ˆæœ"""

import sys
import json
from pathlib import Path
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.llm_client_manager import get_llm_manager
from src.utils.error_handler import ProgressReporter, print_startup_banner


class AntiLLMFuzzDemo:
    """ååˆ¶LLMæ¨¡ç³Šæµ‹è¯•æ¼”ç¤ºå™¨"""
    
    def __init__(self):
        self.llm_manager = get_llm_manager()
        self.results = {}
    
    def run_demo(self, original_file: str, perturbed_files: List[str]):
        """è¿è¡Œå®Œæ•´çš„ååˆ¶æ•ˆæœæ¼”ç¤º"""
        print_startup_banner()
        print("ğŸ¯ ååˆ¶LLMæ¨¡ç³Šæµ‹è¯•æ•ˆæœæ¼”ç¤º")
        print("="*80)
        
        reporter = ProgressReporter(4, "ååˆ¶æ•ˆæœæµ‹è¯•")
        
        # 1. æµ‹è¯•åŸå§‹æ–‡æ¡£
        reporter.start_step("æµ‹è¯•åŸå§‹æ–‡æ¡£çš„LLMç†è§£èƒ½åŠ›")
        original_result = self._test_document_understanding(original_file, "åŸå§‹æ–‡æ¡£")
        reporter.complete_step(True, f"åŸå§‹æ–‡æ¡£æµ‹è¯•å®Œæˆ - ç†è§£åº¦: {original_result['understanding_score']:.1f}%")
        
        # 2. æµ‹è¯•æ‰°åŠ¨æ–‡æ¡£
        reporter.start_step("æµ‹è¯•æ‰°åŠ¨æ–‡æ¡£çš„LLMç†è§£èƒ½åŠ›")
        perturbed_results = []
        for i, perturbed_file in enumerate(perturbed_files[:3]):  # æµ‹è¯•å‰3ä¸ª
            result = self._test_document_understanding(perturbed_file, f"æ‰°åŠ¨æ–‡æ¡£{i+1}")
            perturbed_results.append(result)
        
        avg_perturbed_score = sum(r['understanding_score'] for r in perturbed_results) / len(perturbed_results)
        reporter.complete_step(True, f"æ‰°åŠ¨æ–‡æ¡£æµ‹è¯•å®Œæˆ - å¹³å‡ç†è§£åº¦: {avg_perturbed_score:.1f}%")
        
        # 3. ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å¯¹æ¯”
        reporter.start_step("å¯¹æ¯”æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆèƒ½åŠ›")
        test_case_comparison = self._compare_test_generation(original_file, perturbed_files[0])
        reporter.complete_step(True, "æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå¯¹æ¯”å®Œæˆ")
        
        # 4. åˆ†æååˆ¶æ•ˆæœ
        reporter.start_step("åˆ†æååˆ¶æ•ˆæœ")
        effectiveness = self._analyze_effectiveness(original_result, perturbed_results, test_case_comparison)
        reporter.complete_step(True, f"ååˆ¶æ•ˆæœåˆ†æå®Œæˆ - å¹²æ‰°åº¦: {effectiveness['disruption_rate']:.1f}%")
        
        reporter.finish(True)
        
        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        self._display_results(original_result, perturbed_results, test_case_comparison, effectiveness)
        
        return effectiveness
    
    def _test_document_understanding(self, file_path: str, doc_type: str) -> Dict[str, Any]:
        """æµ‹è¯•LLMå¯¹æ–‡æ¡£çš„ç†è§£èƒ½åŠ›"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æµ‹è¯•1: å…³é”®ä¿¡æ¯æå–
            extraction_prompt = f"""
è¯·åˆ†æä»¥ä¸‹Java APIæ–‡æ¡£ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

{content[:2000]}...

è¯·å›ç­”ï¼š
1. è¿™ä¸ªæ–‡æ¡£æè¿°äº†å“ªäº›ä¸»è¦çš„Javaå®‰å…¨ç›¸å…³APIï¼Ÿ
2. åˆ—å‡ºæ‰€æœ‰æåˆ°çš„å®‰å…¨ç›¸å…³åŒ…å
3. è¯†åˆ«å‡ºè®¤è¯å’Œæˆæƒç›¸å…³çš„åŠŸèƒ½

è¯·ç”¨JSONæ ¼å¼å›ç­”ã€‚
"""
            
            extraction_response = self.llm_manager.generate(extraction_prompt)
            
            # æµ‹è¯•2: ä»£ç ç”Ÿæˆ
            generation_prompt = f"""
åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªä½¿ç”¨Javaå®‰å…¨APIçš„ç¤ºä¾‹ä»£ç ï¼š

{content[:1500]}...

è¦æ±‚ï¼š
1. ä½¿ç”¨æ–‡æ¡£ä¸­æåˆ°çš„å®‰å…¨ç›¸å…³API
2. åŒ…å«è®¤è¯æˆ–åŠ å¯†åŠŸèƒ½
3. ä»£ç è¦å®Œæ•´å¯ç¼–è¯‘
"""
            
            generation_response = self.llm_manager.generate(generation_prompt)
            
            # è¯„ä¼°ç†è§£è´¨é‡
            understanding_score = self._evaluate_understanding(
                content, extraction_response, generation_response
            )
            
            return {
                'doc_type': doc_type,
                'file_path': file_path,
                'understanding_score': understanding_score,
                'extraction_response': extraction_response,
                'generation_response': generation_response,
                'content_length': len(content)
            }
            
        except Exception as e:
            print(f"   âš ï¸ {doc_type}æµ‹è¯•å¤±è´¥: {e}")
            return {
                'doc_type': doc_type,
                'file_path': file_path,
                'understanding_score': 0.0,
                'error': str(e)
            }
    
    def _evaluate_understanding(self, original_content: str, extraction: str, generation: str) -> float:
        """è¯„ä¼°LLMç†è§£è´¨é‡"""
        score = 0.0
        
        # æ£€æŸ¥å…³é”®è¯è¯†åˆ« (40åˆ†)
        security_keywords = [
            'java.security', 'authentication', 'authorization', 'jgss', 'sasl',
            'security', 'credential', 'provider', 'GSS-API', 'SASL'
        ]
        
        found_keywords = 0
        for keyword in security_keywords:
            if keyword.lower() in extraction.lower():
                found_keywords += 1
        
        score += (found_keywords / len(security_keywords)) * 40
        
        # æ£€æŸ¥ä»£ç ç”Ÿæˆè´¨é‡ (30åˆ†)
        code_indicators = ['import', 'class', 'public', 'java.security', 'new ']
        code_quality = 0
        for indicator in code_indicators:
            if indicator in generation:
                code_quality += 1
        
        score += (code_quality / len(code_indicators)) * 30
        
        # æ£€æŸ¥å“åº”å®Œæ•´æ€§ (30åˆ†)
        if len(extraction) > 50:  # æœ‰å®è´¨æ€§å›ç­”
            score += 15
        if len(generation) > 100:  # ç”Ÿæˆäº†ä»£ç 
            score += 15
        
        return min(score, 100.0)
    
    def _compare_test_generation(self, original_file: str, perturbed_file: str) -> Dict[str, Any]:
        """å¯¹æ¯”æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆèƒ½åŠ›"""
        try:
            # è¯»å–æ–‡æ¡£
            with open(original_file, 'r', encoding='utf-8') as f:
                original_content = f.read()
            
            with open(perturbed_file, 'r', encoding='utf-8') as f:
                perturbed_content = f.read()
            
            # ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹çš„æç¤ºè¯
            test_prompt_template = """
åŸºäºä»¥ä¸‹Java APIæ–‡æ¡£ï¼Œç”Ÿæˆ3ä¸ªå®‰å…¨æµ‹è¯•ç”¨ä¾‹ï¼š

{content}

è¦æ±‚ï¼š
1. æµ‹è¯•è®¤è¯åŠŸèƒ½
2. æµ‹è¯•æˆæƒæ£€æŸ¥
3. æµ‹è¯•è¾“å…¥éªŒè¯

æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹åŒ…å«ï¼šæµ‹è¯•ç›®æ ‡ã€è¾“å…¥æ•°æ®ã€æœŸæœ›ç»“æœ
"""
            
            # å¯¹åŸå§‹æ–‡æ¡£ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
            original_tests = self.llm_manager.generate(
                test_prompt_template.format(content=original_content[:2000])
            )
            
            # å¯¹æ‰°åŠ¨æ–‡æ¡£ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
            perturbed_tests = self.llm_manager.generate(
                test_prompt_template.format(content=perturbed_content[:2000])
            )
            
            # åˆ†æå·®å¼‚
            return {
                'original_tests': original_tests,
                'perturbed_tests': perturbed_tests,
                'original_length': len(original_tests),
                'perturbed_length': len(perturbed_tests),
                'quality_degradation': self._calculate_test_quality_degradation(
                    original_tests, perturbed_tests
                )
            }
            
        except Exception as e:
            return {'error': str(e)}
    
    def _calculate_test_quality_degradation(self, original: str, perturbed: str) -> float:
        """è®¡ç®—æµ‹è¯•è´¨é‡ä¸‹é™ç¨‹åº¦"""
        # æ£€æŸ¥æµ‹è¯•ç”¨ä¾‹çš„å…³é”®å…ƒç´ 
        test_elements = ['æµ‹è¯•', 'test', 'éªŒè¯', 'assert', 'è¾“å…¥', 'æœŸæœ›', 'ç»“æœ']
        
        original_score = sum(1 for elem in test_elements if elem in original.lower())
        perturbed_score = sum(1 for elem in test_elements if elem in perturbed.lower())
        
        if original_score == 0:
            return 0.0
        
        degradation = max(0, (original_score - perturbed_score) / original_score * 100)
        return degradation
    
    def _analyze_effectiveness(self, original: Dict, perturbed_list: List[Dict], 
                             test_comparison: Dict) -> Dict[str, Any]:
        """åˆ†æååˆ¶æ•ˆæœ"""
        # è®¡ç®—ç†è§£èƒ½åŠ›ä¸‹é™
        original_score = original.get('understanding_score', 0)
        avg_perturbed_score = sum(p.get('understanding_score', 0) for p in perturbed_list) / len(perturbed_list)
        
        understanding_degradation = max(0, (original_score - avg_perturbed_score) / original_score * 100) if original_score > 0 else 0
        
        # è®¡ç®—æµ‹è¯•ç”Ÿæˆè´¨é‡ä¸‹é™
        test_degradation = test_comparison.get('quality_degradation', 0)
        
        # ç»¼åˆå¹²æ‰°åº¦
        disruption_rate = (understanding_degradation + test_degradation) / 2
        
        return {
            'disruption_rate': disruption_rate,
            'understanding_degradation': understanding_degradation,
            'test_generation_degradation': test_degradation,
            'original_understanding': original_score,
            'perturbed_understanding': avg_perturbed_score,
            'effectiveness_level': self._get_effectiveness_level(disruption_rate)
        }
    
    def _get_effectiveness_level(self, disruption_rate: float) -> str:
        """è·å–æ•ˆæœç­‰çº§"""
        if disruption_rate >= 70:
            return "ğŸ”¥ æé«˜ - ä¸¥é‡å¹²æ‰°LLMç†è§£"
        elif disruption_rate >= 50:
            return "ğŸ¯ é«˜ - æ˜¾è‘—å½±å“LLMæ€§èƒ½"
        elif disruption_rate >= 30:
            return "âš¡ ä¸­ç­‰ - ä¸€å®šç¨‹åº¦å¹²æ‰°"
        elif disruption_rate >= 10:
            return "ğŸ’¡ è½»å¾® - å°‘é‡å½±å“"
        else:
            return "ğŸ˜ æ— æ•ˆ - å‡ ä¹æ— å½±å“"
    
    def _display_results(self, original: Dict, perturbed_list: List[Dict], 
                        test_comparison: Dict, effectiveness: Dict):
        """æ˜¾ç¤ºè¯¦ç»†ç»“æœ"""
        print("\n" + "="*80)
        print("ğŸ¯ ååˆ¶æ•ˆæœåˆ†ææŠ¥å‘Š")
        print("="*80)
        
        # 1. LLMç†è§£èƒ½åŠ›å¯¹æ¯”
        print("\nğŸ“Š LLMæ–‡æ¡£ç†è§£èƒ½åŠ›å¯¹æ¯”:")
        print(f"   åŸå§‹æ–‡æ¡£ç†è§£åº¦: {original.get('understanding_score', 0):.1f}%")
        
        for i, result in enumerate(perturbed_list):
            score = result.get('understanding_score', 0)
            degradation = max(0, original.get('understanding_score', 0) - score)
            print(f"   æ‰°åŠ¨æ–‡æ¡£{i+1}ç†è§£åº¦: {score:.1f}% (ä¸‹é™ {degradation:.1f}%)")
        
        # 2. æµ‹è¯•ç”Ÿæˆèƒ½åŠ›å¯¹æ¯”
        print("\nğŸ§ª æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆèƒ½åŠ›å¯¹æ¯”:")
        if 'quality_degradation' in test_comparison:
            print(f"   æµ‹è¯•è´¨é‡ä¸‹é™: {test_comparison['quality_degradation']:.1f}%")
            print(f"   åŸå§‹æµ‹è¯•é•¿åº¦: {test_comparison.get('original_length', 0)} å­—ç¬¦")
            print(f"   æ‰°åŠ¨æµ‹è¯•é•¿åº¦: {test_comparison.get('perturbed_length', 0)} å­—ç¬¦")
        
        # 3. ç»¼åˆæ•ˆæœè¯„ä¼°
        print("\nğŸ¯ ç»¼åˆååˆ¶æ•ˆæœ:")
        print(f"   æ€»ä½“å¹²æ‰°åº¦: {effectiveness['disruption_rate']:.1f}%")
        print(f"   æ•ˆæœç­‰çº§: {effectiveness['effectiveness_level']}")
        print(f"   ç†è§£èƒ½åŠ›ä¸‹é™: {effectiveness['understanding_degradation']:.1f}%")
        print(f"   æµ‹è¯•ç”Ÿæˆä¸‹é™: {effectiveness['test_generation_degradation']:.1f}%")
        
        # 4. æŠ€æœ¯åˆ†æ
        print("\nğŸ”¬ æŠ€æœ¯åˆ†æ:")
        if effectiveness['disruption_rate'] > 30:
            print("   âœ… æ‰°åŠ¨ç­–ç•¥æœ‰æ•ˆå¹²æ‰°äº†LLMçš„tokenization")
            print("   âœ… æˆåŠŸé™ä½äº†LLMå¯¹å®‰å…¨APIçš„ç†è§£èƒ½åŠ›")
            print("   âœ… å½±å“äº†åŸºäºæ–‡æ¡£çš„æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆè´¨é‡")
        else:
            print("   âš ï¸ æ‰°åŠ¨æ•ˆæœæœ‰é™ï¼Œå¯èƒ½éœ€è¦æ›´å¼ºçš„æ‰°åŠ¨ç­–ç•¥")
        
        # 5. ä½¿ç”¨çš„LLMä¿¡æ¯
        print("\nğŸ¤– æµ‹è¯•ç¯å¢ƒ:")
        print(f"   LLMåç«¯: {self.llm_manager.current_client.name}")
        if self.llm_manager.is_mock_mode():
            print("   âš ï¸ æ³¨æ„: å½“å‰ä½¿ç”¨Mockæ¨¡å¼ï¼Œç»“æœä¸ºæ¨¡æ‹Ÿæ•°æ®")
            print("   ğŸ’¡ å®‰è£…çœŸå®LLMå¯è·å¾—æ›´å‡†ç¡®çš„ååˆ¶æ•ˆæœ")
        
        print("\n" + "="*80)


def main():
    """ä¸»å‡½æ•°"""
    demo = AntiLLMFuzzDemo()
    
    # æŸ¥æ‰¾æœ€æ–°çš„æ‰°åŠ¨æ–‡ä»¶
    output_dirs = list(Path('output').glob('perturbations_*'))
    if not output_dirs:
        print("âŒ æœªæ‰¾åˆ°æ‰°åŠ¨æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ‰°åŠ¨ç”Ÿæˆ")
        print("ğŸ’¡ è¿è¡Œ: python main.py --input data/00java_std.md --strategy semantic")
        return 1
    
    latest_dir = max(output_dirs, key=lambda p: p.stat().st_mtime)
    perturbed_files = list(latest_dir.glob('*.md'))
    
    if not perturbed_files:
        print(f"âŒ åœ¨ {latest_dir} ä¸­æœªæ‰¾åˆ°æ‰°åŠ¨æ–‡ä»¶")
        return 1
    
    print(f"ğŸ” æ‰¾åˆ° {len(perturbed_files)} ä¸ªæ‰°åŠ¨æ–‡ä»¶")
    
    # è¿è¡Œååˆ¶æ•ˆæœæ¼”ç¤º
    original_file = "data/00java_std.md"
    effectiveness = demo.run_demo(original_file, [str(f) for f in perturbed_files])
    
    # ä¿å­˜ç»“æœ
    results_file = f"output/anti_llm_fuzz_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(effectiveness, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    return 0


if __name__ == "__main__":
    from datetime import datetime
    sys.exit(main())